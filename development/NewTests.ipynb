{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "sys.path.append('library/src')\n",
    "import rnnbuilder as ml\n",
    "from rnnbuilder import custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class LinearModule(custom.CustomModule):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def enter_in_shape(self, in_shape):\n",
    "        self.inner = nn.Linear(in_shape[0], self.out_size)\n",
    "\n",
    "    def get_out_shape(self, in_shape):\n",
    "        return (self.out_size,)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        return self.inner(x), h\n",
    "\n",
    "Linear = custom.register_recurrent(module_class=LinearModule, flatten_input=True, single_step=False, unroll_full_state=False)\n",
    "\n",
    "\n",
    "class BellecSpike(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return (input > 0).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * torch.max(torch.zeros([1], device=input.device), 1 - torch.abs(input)) * 0.3\n",
    "\n",
    "class NoResetNeuron(ml.custom.CustomModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.beta = params['BETA']\n",
    "        if params['1-beta'] == 'improved':\n",
    "            self.factor = (1 - self.beta ** 2) ** (0.5)\n",
    "        elif params['1-beta']:\n",
    "            self.factor = (1-self.beta)\n",
    "        else:\n",
    "            self.factor = 1\n",
    "        self.spike_fn = BellecSpike.apply\n",
    "        self.target_var = 1\n",
    "        self.est_rate = 0.5\n",
    "\n",
    "    def get_out_shape(self, in_shape):\n",
    "        return in_shape\n",
    "\n",
    "    def enter_in_shape(self, in_shape):\n",
    "        self.initial_mem = nn.Parameter(torch.zeros(in_shape), requires_grad=True)\n",
    "        self.in_size = in_shape[0]\n",
    "\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        return {\n",
    "            'mem': self.initial_mem.expand([batch_size, self.in_size]),\n",
    "        }\n",
    "\n",
    "    def get_initial_output(self, full_shape):\n",
    "        return self.spike_fn(self.initial_mem.expand(full_shape) - 1)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = {}\n",
    "        new_h['mem'] = self.beta * h['mem'] + self.factor * x\n",
    "        spikes = self.spike_fn(new_h['mem'] - 1)\n",
    "        return spikes, new_h\n",
    "\n",
    "\n",
    "class LIFNeuron(NoResetNeuron):\n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.est_rate = 0.06\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        out, new_h = super().forward(x, h)\n",
    "        new_h['mem'] = new_h['mem'] - out#.detach()#TODO:remove\n",
    "        return out, new_h\n",
    "\n",
    "LIF = custom.register_recurrent(module_class=LIFNeuron, flatten_input=True, single_step=True)\n",
    "\n",
    "\n",
    "class DiscontinuousNeuron(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.spike_fn = BellecSpike.apply\n",
    "        self.threshold = params['THRESHOLD']\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.spike_fn(x-self.threshold)\n",
    "\n",
    "Disc = custom.register_non_recurrent(module_class=DiscontinuousNeuron, flatten_input=False, shape_change=False)\n",
    "\n",
    "class Conv2dModule(ml.custom.CustomModule): # inherit from conv?\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.inner = None\n",
    "\n",
    "    def get_out_shape(self, in_shape):\n",
    "        return None\n",
    "\n",
    "    def enter_in_shape(self, in_shape):\n",
    "        self.inner = nn.Conv2d(in_channels=in_shape[0], *self.args, **self.kwargs)\n",
    "\n",
    "    def forward(self, x, _):\n",
    "        time = x.shape[0]\n",
    "        batch = x.shape[1]\n",
    "        x = x.reshape((time*batch,)+x.shape[2:])\n",
    "        x = self.inner(x)\n",
    "        return x.reshape((time, batch)+x.shape[1:]), ()\n",
    "\n",
    "Conv2d = custom.register_recurrent(module_class=Conv2dModule, single_step=False, flatten_input=False, unroll_full_state=False)\n",
    "\n",
    "\n",
    "class ConvFSModule(ml.custom.CustomModule): # inherit from conv?\n",
    "    defaults = {\n",
    "        'kernel_size': 1,\n",
    "        'stride': 1,\n",
    "        'padding': 0,\n",
    "        'dilation': 1\n",
    "    }\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.kwargs = kwargs\n",
    "        self.inner = None\n",
    "\n",
    "    def enter_in_shape(self, in_shape):\n",
    "        _, changed_params = self.calc_shape(in_shape, self.kwargs)\n",
    "        self.inner = nn.Conv3d(in_channels=in_shape[0], **changed_params)\n",
    "\n",
    "    def get_out_shape(self, in_shape):\n",
    "        out_shape, _ = self.calc_shape(in_shape, self.kwargs)\n",
    "        return out_shape\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_shape(in_shape, params):\n",
    "        depth_params = {**ConvFSModule.defaults}\n",
    "        params = {**ConvFSModule.defaults, **params} #TODO: correct defaults??? was conwrapper2 before\n",
    "        values = {}\n",
    "        if 'frame_stack' in params:\n",
    "            depth_params['kernel_size'] = params['frame_stack']\n",
    "            del params['frame_stack']\n",
    "        for k in depth_params:\n",
    "            if isinstance(params[k], tuple) and len(params[k]) == 2:\n",
    "                params[k] = (depth_params[k], params[k][0], params[k][2])\n",
    "            else:\n",
    "                params[k] = (depth_params[k], params[k], params[k])\n",
    "            values[k] = torch.tensor(params[k][1:], dtype=torch.float)\n",
    "        rest_shape = torch.floor((torch.tensor(in_shape[1:], dtype=torch.float) + 2 * values['padding'] - values['dilation'] * (values['kernel_size'] - 1)-1) / values['stride'] + 1).long()\n",
    "        return tuple([params['out_channels']] + rest_shape.tolist()), params\n",
    "\n",
    "    def forward(self, x, _):\n",
    "        x = x.permute(1, 2, 0, 3, 4)\n",
    "        x = self.inner(x)\n",
    "        return x.permute(2, 0, 1, 3, 4), ()\n",
    "\n",
    "ConvFS = custom.register_recurrent(module_class=ConvFSModule, flatten_input=False, single_step=False, unroll_full_state=False)\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "class HadamardModule(ml.custom.CustomModule):\n",
    "    def get_out_shape(self, in_shapes):\n",
    "        return in_shapes[0]\n",
    "\n",
    "    def forward(self, inputs, _):\n",
    "        return prod(inputs), ()\n",
    "\n",
    "Hadamard = custom.register_recurrent(module_class=HadamardModule, flatten_input=False, single_step=False, unroll_full_state=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#LIF = custom.register_recurrent(module_class=LIFNeuron, prepare_input='flatten', single_step=True, shape_change='none')\n",
    "#Linear = custom.register_non_recurrent(module_class=(lambda in_shape, out_size: nn.Linear(in_shape[0], out_size)),\n",
    "#                              prepare_input='flatten', shape_change=(lambda in_shape, out_size: (out_size,)))\n",
    "#Disc = custom.register_non_recurrent(module_class=DiscontinuousNeuron, prepare_input='flatten', shape_change='none')\n",
    "#ConvFS = custom.register_recurrent(module_class=ConvFSModule, prepare_input='keep', single_step=False, unroll_full_state=False, shape_change='auto')\n",
    "#do fs_conv with dim changes\n",
    "from rnnbuilder.nn import ReLU, Conv2d, Linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "seq = 8\n",
    "batch = 32\n",
    "inp_shape = (1,92,76)\n",
    "example = torch.zeros((seq,batch)+inp_shape)\n",
    "FRAME_STACK = 4\n",
    "N_OUT = 3\n",
    "\n",
    "lif_config = {\n",
    "    'SPIKE_FN' : 'bellec',\n",
    "    'BETA': 0.5,#0.8,\n",
    "    '1-beta': False,\n",
    "}\n",
    "\n",
    "disc_config = {\n",
    "    'SPIKE_FN' : 'bellec',\n",
    "    'THRESHOLD' : 1\n",
    "}\n",
    "\n",
    "'''conv = ConvPath([\n",
    "    Conv3({'out_channels': 32, 'kernel_size': 8, 'stride': 4, 'frame_stack': FRAME_STACK}, CONV_NEURON),\n",
    "    Conv2({'out_channels': 64, 'kernel_size': 4, 'stride': 2}, CONV_NEURON),\n",
    "    Conv2({'out_channels': 64, 'kernel_size': 3, 'stride': 1}, CONV_NEURON)])'''\n",
    "# TODO: neurons\n",
    "conv_neuron = ReLU()#Disc(disc_config)\n",
    "conv_stack = ml.Sequential(ConvFS(out_channels=32, kernel_size=8, stride=4, frame_stack=FRAME_STACK), conv_neuron,\n",
    "                     Conv2d(out_channels=64, kernel_size=4, stride=2), conv_neuron,\n",
    "                     Conv2d(out_channels=64, kernel_size=3, stride=1), conv_neuron)\n",
    "\n",
    "ll = ml.Network()\n",
    "ll.output = ml.Placeholder()\n",
    "ll.output = ll.input.stack(ll.output).apply(Linear(512), LIF(lif_config))#ml.Layer([ll.input, ll.output], [Linear(512), LIF(lif_config)])\n",
    "\n",
    "overall = ml.Sequential(conv_stack, ll, Linear(N_OUT))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = overall.make_model(inp_shape)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from factories_old import Network, Linear, Seq, ExecPath, Conv3, Conv2, ConvPath, Disc, LIF, ReLU, LSTM, PotOut\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "CONV_NEURON = Disc(disc_config) # Seq(LIF(lif_config)) # ReLU()\n",
    "ll_rsnn = Seq(Network(ExecPath(['input', 'output'], [Linear(512), LIF(lif_config)], 'output')))\n",
    "ll_snn = Network(ExecPath(['input'], [Linear(512), Seq(LIF(lif_config))], 'output'))\n",
    "ll_lstm = LSTM(512)\n",
    "ll_ffann = Network(ExecPath(['input'], [Linear(512), ReLU()], 'output'))\n",
    "LAST_LAYER = ll_rsnn\n",
    "conv = ConvPath([\n",
    "    Conv3({'out_channels': 32, 'kernel_size': 8, 'stride': 4, 'frame_stack': FRAME_STACK}, CONV_NEURON),\n",
    "    Conv2({'out_channels': 64, 'kernel_size': 4, 'stride': 2}, CONV_NEURON),\n",
    "    Conv2({'out_channels': 64, 'kernel_size': 3, 'stride': 1}, CONV_NEURON)])\n",
    "new_model_fac = Network(conv, ExecPath(['conv'], [LAST_LAYER, Linear(N_OUT)], 'output'))\n",
    "make_model = lambda: new_model_fac(inp_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "old_model = make_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths.conv.0.conv.conv.weight torch.Size([32, 1, 4, 8, 8])\n",
      "paths.conv.0.conv.conv.bias torch.Size([32])\n",
      "paths.conv.1.conv.conv.weight torch.Size([64, 32, 4, 4])\n",
      "paths.conv.1.conv.conv.bias torch.Size([64])\n",
      "paths.conv.2.conv.conv.weight torch.Size([64, 64, 3, 3])\n",
      "paths.conv.2.conv.conv.bias torch.Size([64])\n",
      "paths.output.0.model.paths.output.0.linear.weight torch.Size([512, 3584])\n",
      "paths.output.0.model.paths.output.0.linear.bias torch.Size([512])\n",
      "paths.output.0.model.paths.output.1.initial_mem torch.Size([512])\n",
      "paths.output.1.linear.weight torch.Size([3, 512])\n",
      "paths.output.1.linear.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, p in old_model.named_parameters():\n",
    "    print(name, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner.mlist.0.mlist.0.inner.inner.weight torch.Size([32, 1, 4, 8, 8])\n",
      "inner.mlist.0.mlist.0.inner.inner.bias torch.Size([32])\n",
      "inner.mlist.0.mlist.2.inner.weight torch.Size([64, 32, 4, 4])\n",
      "inner.mlist.0.mlist.2.inner.bias torch.Size([64])\n",
      "inner.mlist.0.mlist.4.inner.weight torch.Size([64, 64, 3, 3])\n",
      "inner.mlist.0.mlist.4.inner.bias torch.Size([64])\n",
      "inner.mlist.1.layers.c0.layers.output.mlist.0.inner.weight torch.Size([512, 3584])\n",
      "inner.mlist.1.layers.c0.layers.output.mlist.0.inner.bias torch.Size([512])\n",
      "inner.mlist.1.layers.c0.layers.output.mlist.1.inner.initial_mem torch.Size([512])\n",
      "inner.mlist.2.inner.weight torch.Size([3, 512])\n",
      "inner.mlist.2.inner.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name, p.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(len(list(model.named_parameters()))):\n",
    "        par1 = list(model.named_parameters())[i][1]\n",
    "        par1.data = list(old_model.named_parameters())[i][1].data.view(par1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python3.9/site-packages/torch/tensor.py:775: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:848.)\n",
      "  return super(Tensor, self).refine_names(names)\n"
     ]
    }
   ],
   "source": [
    "out, h = model(example)\n",
    "\n",
    "out_old, _ = old_model(example.refine_names('time', 'batch', 'C', 'H', 'W'), old_model.get_initial_state(batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(True)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(out_old.rename(None), out).all()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'rnnbuilder.base' has no attribute 'factories'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-9c19b7ec21bb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbase\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfactories\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLayerBase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'rnnbuilder.base' has no attribute 'factories'"
     ]
    }
   ],
   "source": [
    "ml.base.factories.LayerBase"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n = ml.Network()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n.test = ml.Layer(n.input, [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n.test2 = n.test.stack(n.input).apply([])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n.output = n.test.sum(n.test2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "dir(LIFNeuron)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bool(Conv2dModule.__abstractmethods__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "n.make_model((3,))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}