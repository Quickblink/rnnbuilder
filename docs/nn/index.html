<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>rnnbuilder.nn API documentation</title>
<meta name="description" content="This module provides factories for standard torch modules. Documentation and signatures are directly copied from
PyTorch and copyright applies …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rnnbuilder.nn</code></h1>
</header>
<section id="section-intro">
<p>This module provides factories for standard torch modules. Documentation and signatures are directly copied from
PyTorch and copyright applies accordingly.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34; This module provides factories for standard torch modules. Documentation and signatures are directly copied from
PyTorch and copyright applies accordingly.
&#34;&#34;&#34;
import torch as _torch
from ..base import ModuleFactory as _ModuleFactory
from ..custom._modules import _StatelessWrapper
from ..custom._factories import _NonRecurrentFactory
from ..base._utils import _flatten_shape
from typing import Optional as _Optional

class Linear(_ModuleFactory):
    r&#34;&#34;&#34;Applies a linear transformation to the incoming data: \(y = xA^T + b\)

    Args:
        out_features: size of each output sample
        bias: If set to ``False``, the layer will not learn an additive bias.
            Default: ``True``
    &#34;&#34;&#34;


    def __init__(self, out_features: int, bias: bool = True) -&gt; None:
        super().__init__()
        self.out_features = out_features
        self.bias = bias

    def _assemble_module(self, in_shape, unrolled):
        f_shape = _flatten_shape(in_shape)
        return _StatelessWrapper(f_shape, (self.out_features,), _torch.nn.Linear(f_shape[0], self.out_features, self.bias))

    def _shape_change(self, in_shape):
        return (self.out_features,)


class Conv2d(_NonRecurrentFactory):
    r&#34;&#34;&#34;Applies a 2D convolution over an input signal composed of several input
    planes.


    Args:
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): Zero-padding added to both sides of
            the input. Default: 0
        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,
            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
        groups (int, optional): Number of blocked connections from input
            channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the
            output. Default: ``True``
    &#34;&#34;&#34;
    def __init__(
        self,
        out_channels: int,
        kernel_size: _torch.nn.common_types._size_2_t,
        stride: _torch.nn.common_types._size_2_t = 1,
        padding: _torch.nn.common_types._size_2_t = 0,
        dilation: _torch.nn.common_types._size_2_t = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = &#39;zeros&#39;
    ):
        super().__init__((lambda in_shape, *args: _torch.nn.Conv2d(in_shape[0], *args)), False, &#39;auto&#39;,
                         out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)

class ReLU(_ModuleFactory):
    r&#34;&#34;&#34;Applies the rectified linear unit function element-wise:

    .. math::
        \text{ReLU}(x) = (x)^+ = \max(0, x)

    Args:
        inplace: can optionally do the operation in-place. Default: ``False``

    &#34;&#34;&#34;

    def __init__(self, inplace: bool = False):
        super().__init__()
        self.inplace = inplace

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.ReLU(inplace=self.inplace))


class Sigmoid(_ModuleFactory):
    r&#34;&#34;&#34;Applies the element-wise function:

    .. math::
        \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}
    &#34;&#34;&#34;

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.Sigmoid())


class Tanh(_ModuleFactory):
    r&#34;&#34;&#34;Applies the element-wise function:

    .. math::
        \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}
    &#34;&#34;&#34;

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.Tanh())



class BatchNorm2d(_ModuleFactory):
    r&#34;&#34;&#34;Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
    with additional channel dimension) as described in the paper
    [Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift](https://arxiv.org/abs/1502.03167) .

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension over
    the mini-batches and \(\gamma\) and \(\beta\) are learnable parameter vectors
    of size `C` (where `C` is the input size). By default, the elements of \(\gamma\) are set
    to 1 and the elements of \(\beta\) are set to 0. The standard-deviation is calculated
    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.


    Because the Batch Normalization is done over the `C` dimension, computing statistics
    on `(N, H, W)` slices, it&#39;s common terminology to call this Spatial Batch Normalization.

    Args:
        eps: a value added to the denominator for numerical stability.
            Default: 1e-5
        momentum: the value used for the running_mean and running_var
            computation. Can be set to ``None`` for cumulative moving average
            (i.e. simple average). Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters. Default: ``True``
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics, and initializes statistics
            buffers `running_mean` and `running_var` as ``None``.
            When these buffers are ``None``, this module always uses batch statistics.
            in both training and eval modes. Default: ``True``
    &#34;&#34;&#34;

    def __init__(self, eps=1e-5, momentum=0.1, affine=True,
                 track_running_stats=True):
        self.args = eps, momentum, affine, track_running_stats

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.BatchNorm2d(in_shape[0], *self.args))


class MaxPool2d(_NonRecurrentFactory):
    r&#34;&#34;&#34;Applies a 2D max pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size \((N, C, H, W)\),
    output \((N, C, H_{out}, W_{out})\) and `kernel_size` \((kH, kW)\)
    can be precisely described as:

    .. math::
        \begin{aligned}
            out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                    &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                                   \text{stride[1]} \times w + n)
        \end{aligned}

    If `padding` is non-zero, then the input is implicitly zero-padded on both sides
    for `padding` number of points. `dilation` controls the spacing between the kernel points.

    The parameters can either be:

    - a single ``int`` -- in which case the same value is used for the height and width dimension
    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
      and the second `int` for the width dimension

    Args:
        kernel_size: the size of the window to take a max over
        stride: the stride of the window. Default value is `kernel_size`
        padding: implicit zero padding to be added on both sides
        dilation: a parameter that controls the stride of elements in the window

    &#34;&#34;&#34;

    def __init__(self, kernel_size: _torch.nn.common_types._size_any_t, stride: _Optional[_torch.nn.common_types._size_any_t] = None,
                 padding: _torch.nn.common_types._size_any_t = 0, dilation: _torch.nn.common_types._size_any_t = 1) -&gt; None:
        super().__init__((lambda in_shape, *args: _torch.nn.MaxPool2d(*args)), False, &#39;auto&#39;,
                         kernel_size, stride, padding, dilation)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rnnbuilder.nn.BatchNorm2d"><code class="flex name class">
<span>class <span class="ident">BatchNorm2d</span></span>
<span>(</span><span>eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
with additional channel dimension) as described in the paper
<a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> .</p>
<p><span><span class="MathJax_Preview"> y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta </span><script type="math/tex; mode=display"> y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta </script></span>
The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> and <span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> are learnable parameter vectors
of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> are set
to 1 and the elements of <span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> are set to 0. The standard-deviation is calculated
via the biased estimator, equivalent to <code>torch.var(input, unbiased=False)</code>.</p>
<p>Because the Batch Normalization is done over the <code>C</code> dimension, computing statistics
on <code>(N, H, W)</code> slices, it's common terminology to call this Spatial Batch Normalization.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong></dt>
<dd>a value added to the denominator for numerical stability.
Default: 1e-5</dd>
<dt><strong><code>momentum</code></strong></dt>
<dd>the value used for the running_mean and running_var
computation. Can be set to <code>None</code> for cumulative moving average
(i.e. simple average). Default: 0.1</dd>
<dt><strong><code>affine</code></strong></dt>
<dd>a boolean value that when set to <code>True</code>, this module has
learnable affine parameters. Default: <code>True</code></dd>
<dt><strong><code>track_running_stats</code></strong></dt>
<dd>a boolean value that when set to <code>True</code>, this
module tracks the running mean and variance, and when set to <code>False</code>,
this module does not track such statistics, and initializes statistics
buffers <code>running_mean</code> and <code>running_var</code> as <code>None</code>.
When these buffers are <code>None</code>, this module always uses batch statistics.
in both training and eval modes. Default: <code>True</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchNorm2d(_ModuleFactory):
    r&#34;&#34;&#34;Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
    with additional channel dimension) as described in the paper
    [Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift](https://arxiv.org/abs/1502.03167) .

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension over
    the mini-batches and \(\gamma\) and \(\beta\) are learnable parameter vectors
    of size `C` (where `C` is the input size). By default, the elements of \(\gamma\) are set
    to 1 and the elements of \(\beta\) are set to 0. The standard-deviation is calculated
    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.


    Because the Batch Normalization is done over the `C` dimension, computing statistics
    on `(N, H, W)` slices, it&#39;s common terminology to call this Spatial Batch Normalization.

    Args:
        eps: a value added to the denominator for numerical stability.
            Default: 1e-5
        momentum: the value used for the running_mean and running_var
            computation. Can be set to ``None`` for cumulative moving average
            (i.e. simple average). Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters. Default: ``True``
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics, and initializes statistics
            buffers `running_mean` and `running_var` as ``None``.
            When these buffers are ``None``, this module always uses batch statistics.
            in both training and eval modes. Default: ``True``
    &#34;&#34;&#34;

    def __init__(self, eps=1e-5, momentum=0.1, affine=True,
                 track_running_stats=True):
        self.args = eps, momentum, affine, track_running_stats

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.BatchNorm2d(in_shape[0], *self.args))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rnnbuilder.nn.Conv2d"><code class="flex name class">
<span>class <span class="ident">Conv2d</span></span>
<span>(</span><span>out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = 1, padding: Union[int, Tuple[int, int]] = 0, dilation: Union[int, Tuple[int, int]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')</span>
</code></dt>
<dd>
<div class="desc"><p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>out_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of channels produced by the convolution</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code> or <code>tuple</code></dt>
<dd>Size of the convolving kernel</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code> or <code>tuple</code>, optional</dt>
<dd>Stride of the convolution. Default: 1</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code> or <code>tuple</code>, optional</dt>
<dd>Zero-padding added to both sides of
the input. Default: 0</dd>
<dt><strong><code>padding_mode</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd><code>'zeros'</code>, <code>'reflect'</code>,
<code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code> or <code>tuple</code>, optional</dt>
<dd>Spacing between kernel elements. Default: 1</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of blocked connections from input
channels to output channels. Default: 1</dd>
<dt><strong><code>bias</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code>, adds a learnable bias to the
output. Default: <code>True</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Conv2d(_NonRecurrentFactory):
    r&#34;&#34;&#34;Applies a 2D convolution over an input signal composed of several input
    planes.


    Args:
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): Zero-padding added to both sides of
            the input. Default: 0
        padding_mode (string, optional): ``&#39;zeros&#39;``, ``&#39;reflect&#39;``,
            ``&#39;replicate&#39;`` or ``&#39;circular&#39;``. Default: ``&#39;zeros&#39;``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
        groups (int, optional): Number of blocked connections from input
            channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the
            output. Default: ``True``
    &#34;&#34;&#34;
    def __init__(
        self,
        out_channels: int,
        kernel_size: _torch.nn.common_types._size_2_t,
        stride: _torch.nn.common_types._size_2_t = 1,
        padding: _torch.nn.common_types._size_2_t = 0,
        dilation: _torch.nn.common_types._size_2_t = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = &#39;zeros&#39;
    ):
        super().__init__((lambda in_shape, *args: _torch.nn.Conv2d(in_shape[0], *args)), False, &#39;auto&#39;,
                         out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>rnnbuilder.custom._factories._NonRecurrentFactory</li>
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rnnbuilder.nn.Linear"><code class="flex name class">
<span>class <span class="ident">Linear</span></span>
<span>(</span><span>out_features: int, bias: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies a linear transformation to the incoming data: <span><span class="MathJax_Preview">y = xA^T + b</span><script type="math/tex">y = xA^T + b</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>out_features</code></strong></dt>
<dd>size of each output sample</dd>
<dt><strong><code>bias</code></strong></dt>
<dd>If set to <code>False</code>, the layer will not learn an additive bias.
Default: <code>True</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Linear(_ModuleFactory):
    r&#34;&#34;&#34;Applies a linear transformation to the incoming data: \(y = xA^T + b\)

    Args:
        out_features: size of each output sample
        bias: If set to ``False``, the layer will not learn an additive bias.
            Default: ``True``
    &#34;&#34;&#34;


    def __init__(self, out_features: int, bias: bool = True) -&gt; None:
        super().__init__()
        self.out_features = out_features
        self.bias = bias

    def _assemble_module(self, in_shape, unrolled):
        f_shape = _flatten_shape(in_shape)
        return _StatelessWrapper(f_shape, (self.out_features,), _torch.nn.Linear(f_shape[0], self.out_features, self.bias))

    def _shape_change(self, in_shape):
        return (self.out_features,)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rnnbuilder.nn.MaxPool2d"><code class="flex name class">
<span>class <span class="ident">MaxPool2d</span></span>
<span>(</span><span>kernel_size: Union[int, Tuple[int, ...]], stride: Union[int, Tuple[int, ...], NoneType] = None, padding: Union[int, Tuple[int, ...]] = 0, dilation: Union[int, Tuple[int, ...]] = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies a 2D max pooling over an input signal composed of several input
planes.</p>
<p>In the simplest case, the output value of the layer with input size <span><span class="MathJax_Preview">(N, C, H, W)</span><script type="math/tex">(N, C, H, W)</script></span>,
output <span><span class="MathJax_Preview">(N, C, H_{out}, W_{out})</span><script type="math/tex">(N, C, H_{out}, W_{out})</script></span> and <code>kernel_size</code> <span><span class="MathJax_Preview">(kH, kW)</span><script type="math/tex">(kH, kW)</script></span>
can be precisely described as:</p>
<p><span><span class="MathJax_Preview"> \begin{aligned}
out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
&amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
\text{stride[1]} \times w + n)
\end{aligned} </span><script type="math/tex; mode=display"> \begin{aligned}
out(N_i, C_j, h, w) ={} & \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
& \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
\text{stride[1]} \times w + n)
\end{aligned} </script></span>
If <code>padding</code> is non-zero, then the input is implicitly zero-padded on both sides
for <code>padding</code> number of points. <code>dilation</code> controls the spacing between the kernel points.</p>
<p>The parameters can either be:</p>
<ul>
<li>a single <code>int</code> &ndash; in which case the same value is used for the height and width dimension</li>
<li>a <code>tuple</code> of two ints &ndash; in which case, the first <code>int</code> is used for the height dimension,
and the second <code>int</code> for the width dimension</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kernel_size</code></strong></dt>
<dd>the size of the window to take a max over</dd>
<dt><strong><code>stride</code></strong></dt>
<dd>the stride of the window. Default value is <code>kernel_size</code></dd>
<dt><strong><code>padding</code></strong></dt>
<dd>implicit zero padding to be added on both sides</dd>
<dt><strong><code>dilation</code></strong></dt>
<dd>a parameter that controls the stride of elements in the window</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxPool2d(_NonRecurrentFactory):
    r&#34;&#34;&#34;Applies a 2D max pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size \((N, C, H, W)\),
    output \((N, C, H_{out}, W_{out})\) and `kernel_size` \((kH, kW)\)
    can be precisely described as:

    .. math::
        \begin{aligned}
            out(N_i, C_j, h, w) ={} &amp; \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                    &amp; \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                                   \text{stride[1]} \times w + n)
        \end{aligned}

    If `padding` is non-zero, then the input is implicitly zero-padded on both sides
    for `padding` number of points. `dilation` controls the spacing between the kernel points.

    The parameters can either be:

    - a single ``int`` -- in which case the same value is used for the height and width dimension
    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
      and the second `int` for the width dimension

    Args:
        kernel_size: the size of the window to take a max over
        stride: the stride of the window. Default value is `kernel_size`
        padding: implicit zero padding to be added on both sides
        dilation: a parameter that controls the stride of elements in the window

    &#34;&#34;&#34;

    def __init__(self, kernel_size: _torch.nn.common_types._size_any_t, stride: _Optional[_torch.nn.common_types._size_any_t] = None,
                 padding: _torch.nn.common_types._size_any_t = 0, dilation: _torch.nn.common_types._size_any_t = 1) -&gt; None:
        super().__init__((lambda in_shape, *args: _torch.nn.MaxPool2d(*args)), False, &#39;auto&#39;,
                         kernel_size, stride, padding, dilation)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>rnnbuilder.custom._factories._NonRecurrentFactory</li>
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rnnbuilder.nn.ReLU"><code class="flex name class">
<span>class <span class="ident">ReLU</span></span>
<span>(</span><span>inplace: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the rectified linear unit function element-wise:</p>
<p><span><span class="MathJax_Preview"> \text{ReLU}(x) = (x)^+ = \max(0, x) </span><script type="math/tex; mode=display"> \text{ReLU}(x) = (x)^+ = \max(0, x) </script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inplace</code></strong></dt>
<dd>can optionally do the operation in-place. Default: <code>False</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReLU(_ModuleFactory):
    r&#34;&#34;&#34;Applies the rectified linear unit function element-wise:

    .. math::
        \text{ReLU}(x) = (x)^+ = \max(0, x)

    Args:
        inplace: can optionally do the operation in-place. Default: ``False``

    &#34;&#34;&#34;

    def __init__(self, inplace: bool = False):
        super().__init__()
        self.inplace = inplace

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.ReLU(inplace=self.inplace))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rnnbuilder.nn.Sigmoid"><code class="flex name class">
<span>class <span class="ident">Sigmoid</span></span>
</code></dt>
<dd>
<div class="desc"><p>Applies the element-wise function:</p>
<p><span><span class="MathJax_Preview"> \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)} </span><script type="math/tex; mode=display"> \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sigmoid(_ModuleFactory):
    r&#34;&#34;&#34;Applies the element-wise function:

    .. math::
        \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}
    &#34;&#34;&#34;

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.Sigmoid())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="rnnbuilder.nn.Tanh"><code class="flex name class">
<span>class <span class="ident">Tanh</span></span>
</code></dt>
<dd>
<div class="desc"><p>Applies the element-wise function:</p>
<p><span><span class="MathJax_Preview"> \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)} </span><script type="math/tex; mode=display"> \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)} </script></span></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tanh(_ModuleFactory):
    r&#34;&#34;&#34;Applies the element-wise function:

    .. math::
        \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}
    &#34;&#34;&#34;

    def _assemble_module(self, in_shape, unrolled):
        return _StatelessWrapper(in_shape, in_shape, _torch.nn.Tanh())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="rnnbuilder.base.ModuleFactory" href="../base/index.html#rnnbuilder.base.ModuleFactory">ModuleFactory</a></b></code>:
<ul class="hlist">
<li><code><a title="rnnbuilder.base.ModuleFactory.make_model" href="../base/index.html#rnnbuilder.base.ModuleFactory.make_model">make_model</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rnnbuilder" href="../index.html">rnnbuilder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rnnbuilder.nn.BatchNorm2d" href="#rnnbuilder.nn.BatchNorm2d">BatchNorm2d</a></code></h4>
</li>
<li>
<h4><code><a title="rnnbuilder.nn.Conv2d" href="#rnnbuilder.nn.Conv2d">Conv2d</a></code></h4>
</li>
<li>
<h4><code><a title="rnnbuilder.nn.Linear" href="#rnnbuilder.nn.Linear">Linear</a></code></h4>
</li>
<li>
<h4><code><a title="rnnbuilder.nn.MaxPool2d" href="#rnnbuilder.nn.MaxPool2d">MaxPool2d</a></code></h4>
</li>
<li>
<h4><code><a title="rnnbuilder.nn.ReLU" href="#rnnbuilder.nn.ReLU">ReLU</a></code></h4>
</li>
<li>
<h4><code><a title="rnnbuilder.nn.Sigmoid" href="#rnnbuilder.nn.Sigmoid">Sigmoid</a></code></h4>
</li>
<li>
<h4><code><a title="rnnbuilder.nn.Tanh" href="#rnnbuilder.nn.Tanh">Tanh</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>