<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>rnnbuilder.custom API documentation</title>
<meta name="description" content="Functions to extend this library with custom modules" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>rnnbuilder.custom</code></h1>
</header>
<section id="section-intro">
<p>Functions to extend this library with custom modules</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Functions to extend this library with custom modules&#34;&#34;&#34;
from typing import Type
import torch as _torch
from ._factories import _NonRecurrentFactory, _RecurrentFactory
from abc import ABC as _ABC, abstractmethod as _abstractmethod

__pdoc__ = {&#39;CustomModule.training&#39; : False, &#39;CustomModule.dump_patches&#39; : False}


class CustomModule(_torch.nn.Module, _ABC):
    &#34;&#34;&#34;Abstract base class for custom recurrent modules. Inheriting classes must be registered with `register_recurrent`
    to retrieve a corresponding factory class. The factory class will initialize with the same parameters as the
    registered CustomModule.


    &#34;&#34;&#34;
    def enter_in_shape(self, in_shape):
        &#34;&#34;&#34;Gets called immediately after initialization. Overwrite if internal parameters depend on the input shape.
        &#34;&#34;&#34;
        pass

    @_abstractmethod
    def get_out_shape(self, in_shape):
        &#34;&#34;&#34;
        Args:
            in_shape: data shape of incoming tensor (excluding time and batch dimensions) before any flattening is
                applied

        In most cases, one of three values should be returned:
        1. &#39;in_shape&#39; if the module does not perform changes to the shape of the data (flattening does not count)
        2. some_fixed_shape if the output shape is independent of the input e.g. (out_shape,) for Linear layers
        3. &#39;None&#39; in all other cases (output shape will be inferred automatically)
        &#34;&#34;&#34;
        pass

    def get_initial_output(self, full_shape):
        &#34;&#34;&#34;Returns the initial output used for `Placeholder`s. This defaults to zero and can be overwritten by
        individual `Placeholder`s.

        Args:
            full_shape: shape of the output in the format (time, batch, data0, data1, ...). The time dimension will
                always be 1.
        &#34;&#34;&#34;
        return _torch.zeros(full_shape, device=self.device)

    @_abstractmethod
    def forward(self, input, state):
        &#34;&#34;&#34;Must be implemented with the following signature.
        Args:
            input: the input tensor, the expected shape needs to be reported when registering the module, see
                `register_recurrent`
            state: some state as any combination of dicts, lists, tuples and tensors (needs to have format consistent
                with `get_initial_state`). The empty tuple is used to indicate no state.

        Returns:
            (output, new_state) where
            output: the output tensor, format needs to match input but data dimensions can be different
            new_state: the new state in the same format as the input state
            &#34;&#34;&#34;
        pass

    def get_initial_state(self, batch_size):
        &#34;&#34;&#34;Returns initial state (in the same format as `forward`, for a single step, first dimension being batch size).
        &#34;&#34;&#34;
        return ()

class _NonRecurrentGenerator:
    def __init__(self, make_module, prepare_input, shape_change):
        self._make_module = make_module
        self._shape_change = shape_change
        self._prepare_input = prepare_input

    def __call__(self, *args, **kwargs):
        return _NonRecurrentFactory(self._make_module, self._prepare_input, self._shape_change, *args, **kwargs)


def register_non_recurrent(*, module_class: Type[_torch.nn.Module],
                           flatten_input: bool,
                           shape_change: bool):
    &#34;&#34;&#34;Register a (non-recurrent) _torch._torch.nn.Module to retrieve a factory class. The factory class will initialize with
    the same parameters as the registered Module. If this interface is too restrictive, wrap the Module in a
    `CustomModule` and use `register_recurrent` instead.

    Args:
        module_class: a class derived from _torch._torch.nn.Module. The forward method needs to conform to a fixed signature.
            It must accept a single input tensor of format (batch, data0, data1, ...) and return a single output tensor.
        flatten_input: If True, the input is flattened to shape (batch, data0*data1*...) before given to the module.
        shape_change: Indicate whether the module changes the shape of the tensor going through, i.e. put False if input
            (after the optional flatten) and output shapes of the module are identical otherwise True
    &#34;&#34;&#34;
    initializer = (lambda in_shape, *args, **kwargs: module_class(*args, **kwargs))\
        if type(module_class) is type else module_class #TODO: remove initializer
    return _NonRecurrentGenerator(initializer, &#39;flatten&#39; if flatten_input else &#39;keep&#39;,
                                  &#39;auto&#39; if shape_change else &#39;none&#39;)


class _RecurrentGenerator:
    def __init__(self, module_class, prepare_input, single_step, unroll_full_state):
        self._module_class = module_class
        self._prepare_input = prepare_input
        self._single_step = single_step
        self._unroll_full_state = unroll_full_state

    def __call__(self, *args, **kwargs):
        return _RecurrentFactory(self._module_class, self._prepare_input, self._single_step, self._unroll_full_state,
                                 *args, **kwargs)

def register_recurrent(*, module_class: Type[CustomModule], flatten_input: bool,
                       single_step: bool, unroll_full_state: bool = True):
    &#34;&#34;&#34;Register a (possibly recurrent) `CustomModule` to retrieve a factory class. The factory class will initialize
    with the same parameters as the registered `CustomModule`.

    Args:
        module_class: a class derived from `CustomModule`
        flatten_input: If True, the input is flattened to a single data dimension before given to the module.
        single_step: If True, for each time step the forward method of the module will be invoked with a tensor of
            format (batch, data_shape). If False, it will be only invoked once with a tensor of format
            (time, batch, data_shape).
        unroll_full_state: (_Optional) If True and the state for every time step is required (full_state mode), the
            module will be invoked for each time step even if single_step=False. If False, the module has to check for
            self._full_state and return a sequence of states appropriately
    &#34;&#34;&#34;
    if module_class.__abstractmethods__:
        raise Exception(&#34;Can&#39;t register &#34;+module_class.__name__+&#34;. All abstract methods need to be overwritten.&#34;)
    return _RecurrentGenerator(module_class, &#39;flatten&#39; if flatten_input else &#39;keep&#39;, single_step, unroll_full_state)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="rnnbuilder.custom.register_non_recurrent"><code class="name flex">
<span>def <span class="ident">register_non_recurrent</span></span>(<span>*, module_class: Type[torch.nn.modules.module.Module], flatten_input: bool, shape_change: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Register a (non-recurrent) _torch._torch.nn.Module to retrieve a factory class. The factory class will initialize with
the same parameters as the registered Module. If this interface is too restrictive, wrap the Module in a
<code><a title="rnnbuilder.custom.CustomModule" href="#rnnbuilder.custom.CustomModule">CustomModule</a></code> and use <code><a title="rnnbuilder.custom.register_recurrent" href="#rnnbuilder.custom.register_recurrent">register_recurrent()</a></code> instead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>module_class</code></strong></dt>
<dd>a class derived from _torch._torch.nn.Module. The forward method needs to conform to a fixed signature.
It must accept a single input tensor of format (batch, data0, data1, &hellip;) and return a single output tensor.</dd>
<dt><strong><code>flatten_input</code></strong></dt>
<dd>If True, the input is flattened to shape (batch, data0<em>data1</em>&hellip;) before given to the module.</dd>
<dt><strong><code>shape_change</code></strong></dt>
<dd>Indicate whether the module changes the shape of the tensor going through, i.e. put False if input
(after the optional flatten) and output shapes of the module are identical otherwise True</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_non_recurrent(*, module_class: Type[_torch.nn.Module],
                           flatten_input: bool,
                           shape_change: bool):
    &#34;&#34;&#34;Register a (non-recurrent) _torch._torch.nn.Module to retrieve a factory class. The factory class will initialize with
    the same parameters as the registered Module. If this interface is too restrictive, wrap the Module in a
    `CustomModule` and use `register_recurrent` instead.

    Args:
        module_class: a class derived from _torch._torch.nn.Module. The forward method needs to conform to a fixed signature.
            It must accept a single input tensor of format (batch, data0, data1, ...) and return a single output tensor.
        flatten_input: If True, the input is flattened to shape (batch, data0*data1*...) before given to the module.
        shape_change: Indicate whether the module changes the shape of the tensor going through, i.e. put False if input
            (after the optional flatten) and output shapes of the module are identical otherwise True
    &#34;&#34;&#34;
    initializer = (lambda in_shape, *args, **kwargs: module_class(*args, **kwargs))\
        if type(module_class) is type else module_class #TODO: remove initializer
    return _NonRecurrentGenerator(initializer, &#39;flatten&#39; if flatten_input else &#39;keep&#39;,
                                  &#39;auto&#39; if shape_change else &#39;none&#39;)</code></pre>
</details>
</dd>
<dt id="rnnbuilder.custom.register_recurrent"><code class="name flex">
<span>def <span class="ident">register_recurrent</span></span>(<span>*, module_class: Type[<a title="rnnbuilder.custom.CustomModule" href="#rnnbuilder.custom.CustomModule">CustomModule</a>], flatten_input: bool, single_step: bool, unroll_full_state: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Register a (possibly recurrent) <code><a title="rnnbuilder.custom.CustomModule" href="#rnnbuilder.custom.CustomModule">CustomModule</a></code> to retrieve a factory class. The factory class will initialize
with the same parameters as the registered <code><a title="rnnbuilder.custom.CustomModule" href="#rnnbuilder.custom.CustomModule">CustomModule</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>module_class</code></strong></dt>
<dd>a class derived from <code><a title="rnnbuilder.custom.CustomModule" href="#rnnbuilder.custom.CustomModule">CustomModule</a></code></dd>
<dt><strong><code>flatten_input</code></strong></dt>
<dd>If True, the input is flattened to a single data dimension before given to the module.</dd>
<dt><strong><code>single_step</code></strong></dt>
<dd>If True, for each time step the forward method of the module will be invoked with a tensor of
format (batch, data_shape). If False, it will be only invoked once with a tensor of format
(time, batch, data_shape).</dd>
<dt><strong><code>unroll_full_state</code></strong></dt>
<dd>(_Optional) If True and the state for every time step is required (full_state mode), the
module will be invoked for each time step even if single_step=False. If False, the module has to check for
self._full_state and return a sequence of states appropriately</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_recurrent(*, module_class: Type[CustomModule], flatten_input: bool,
                       single_step: bool, unroll_full_state: bool = True):
    &#34;&#34;&#34;Register a (possibly recurrent) `CustomModule` to retrieve a factory class. The factory class will initialize
    with the same parameters as the registered `CustomModule`.

    Args:
        module_class: a class derived from `CustomModule`
        flatten_input: If True, the input is flattened to a single data dimension before given to the module.
        single_step: If True, for each time step the forward method of the module will be invoked with a tensor of
            format (batch, data_shape). If False, it will be only invoked once with a tensor of format
            (time, batch, data_shape).
        unroll_full_state: (_Optional) If True and the state for every time step is required (full_state mode), the
            module will be invoked for each time step even if single_step=False. If False, the module has to check for
            self._full_state and return a sequence of states appropriately
    &#34;&#34;&#34;
    if module_class.__abstractmethods__:
        raise Exception(&#34;Can&#39;t register &#34;+module_class.__name__+&#34;. All abstract methods need to be overwritten.&#34;)
    return _RecurrentGenerator(module_class, &#39;flatten&#39; if flatten_input else &#39;keep&#39;, single_step, unroll_full_state)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="rnnbuilder.custom.CustomModule"><code class="flex name class">
<span>class <span class="ident">CustomModule</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for custom recurrent modules. Inheriting classes must be registered with <code><a title="rnnbuilder.custom.register_recurrent" href="#rnnbuilder.custom.register_recurrent">register_recurrent()</a></code>
to retrieve a corresponding factory class. The factory class will initialize with the same parameters as the
registered CustomModule.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomModule(_torch.nn.Module, _ABC):
    &#34;&#34;&#34;Abstract base class for custom recurrent modules. Inheriting classes must be registered with `register_recurrent`
    to retrieve a corresponding factory class. The factory class will initialize with the same parameters as the
    registered CustomModule.


    &#34;&#34;&#34;
    def enter_in_shape(self, in_shape):
        &#34;&#34;&#34;Gets called immediately after initialization. Overwrite if internal parameters depend on the input shape.
        &#34;&#34;&#34;
        pass

    @_abstractmethod
    def get_out_shape(self, in_shape):
        &#34;&#34;&#34;
        Args:
            in_shape: data shape of incoming tensor (excluding time and batch dimensions) before any flattening is
                applied

        In most cases, one of three values should be returned:
        1. &#39;in_shape&#39; if the module does not perform changes to the shape of the data (flattening does not count)
        2. some_fixed_shape if the output shape is independent of the input e.g. (out_shape,) for Linear layers
        3. &#39;None&#39; in all other cases (output shape will be inferred automatically)
        &#34;&#34;&#34;
        pass

    def get_initial_output(self, full_shape):
        &#34;&#34;&#34;Returns the initial output used for `Placeholder`s. This defaults to zero and can be overwritten by
        individual `Placeholder`s.

        Args:
            full_shape: shape of the output in the format (time, batch, data0, data1, ...). The time dimension will
                always be 1.
        &#34;&#34;&#34;
        return _torch.zeros(full_shape, device=self.device)

    @_abstractmethod
    def forward(self, input, state):
        &#34;&#34;&#34;Must be implemented with the following signature.
        Args:
            input: the input tensor, the expected shape needs to be reported when registering the module, see
                `register_recurrent`
            state: some state as any combination of dicts, lists, tuples and tensors (needs to have format consistent
                with `get_initial_state`). The empty tuple is used to indicate no state.

        Returns:
            (output, new_state) where
            output: the output tensor, format needs to match input but data dimensions can be different
            new_state: the new state in the same format as the input state
            &#34;&#34;&#34;
        pass

    def get_initial_state(self, batch_size):
        &#34;&#34;&#34;Returns initial state (in the same format as `forward`, for a single step, first dimension being batch size).
        &#34;&#34;&#34;
        return ()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>rnnbuilder.rnn._modules._LSTMModule</li>
<li>rnnbuilder.rnn._modules._TempConvPlus2dModule</li>
<li>rnnbuilder.snn._modules._NoResetNeuron</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="rnnbuilder.custom.CustomModule.enter_in_shape"><code class="name flex">
<span>def <span class="ident">enter_in_shape</span></span>(<span>self, in_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets called immediately after initialization. Overwrite if internal parameters depend on the input shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enter_in_shape(self, in_shape):
    &#34;&#34;&#34;Gets called immediately after initialization. Overwrite if internal parameters depend on the input shape.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="rnnbuilder.custom.CustomModule.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input, state) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Must be implemented with the following signature.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong></dt>
<dd>the input tensor, the expected shape needs to be reported when registering the module, see
<code><a title="rnnbuilder.custom.register_recurrent" href="#rnnbuilder.custom.register_recurrent">register_recurrent()</a></code></dd>
<dt><strong><code>state</code></strong></dt>
<dd>some state as any combination of dicts, lists, tuples and tensors (needs to have format consistent
with <code>get_initial_state</code>). The empty tuple is used to indicate no state.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>(output, new_state) where</dt>
<dt><code>output</code></dt>
<dd>the output tensor, format needs to match input but data dimensions can be different</dd>
<dt><code>new_state</code></dt>
<dd>the new state in the same format as the input state</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_abstractmethod
def forward(self, input, state):
    &#34;&#34;&#34;Must be implemented with the following signature.
    Args:
        input: the input tensor, the expected shape needs to be reported when registering the module, see
            `register_recurrent`
        state: some state as any combination of dicts, lists, tuples and tensors (needs to have format consistent
            with `get_initial_state`). The empty tuple is used to indicate no state.

    Returns:
        (output, new_state) where
        output: the output tensor, format needs to match input but data dimensions can be different
        new_state: the new state in the same format as the input state
        &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="rnnbuilder.custom.CustomModule.get_initial_output"><code class="name flex">
<span>def <span class="ident">get_initial_output</span></span>(<span>self, full_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the initial output used for <code>Placeholder</code>s. This defaults to zero and can be overwritten by
individual <code>Placeholder</code>s.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>full_shape</code></strong></dt>
<dd>shape of the output in the format (time, batch, data0, data1, &hellip;). The time dimension will
always be 1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_initial_output(self, full_shape):
    &#34;&#34;&#34;Returns the initial output used for `Placeholder`s. This defaults to zero and can be overwritten by
    individual `Placeholder`s.

    Args:
        full_shape: shape of the output in the format (time, batch, data0, data1, ...). The time dimension will
            always be 1.
    &#34;&#34;&#34;
    return _torch.zeros(full_shape, device=self.device)</code></pre>
</details>
</dd>
<dt id="rnnbuilder.custom.CustomModule.get_initial_state"><code class="name flex">
<span>def <span class="ident">get_initial_state</span></span>(<span>self, batch_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns initial state (in the same format as <code>forward</code>, for a single step, first dimension being batch size).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_initial_state(self, batch_size):
    &#34;&#34;&#34;Returns initial state (in the same format as `forward`, for a single step, first dimension being batch size).
    &#34;&#34;&#34;
    return ()</code></pre>
</details>
</dd>
<dt id="rnnbuilder.custom.CustomModule.get_out_shape"><code class="name flex">
<span>def <span class="ident">get_out_shape</span></span>(<span>self, in_shape)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>in_shape</code></strong></dt>
<dd>data shape of incoming tensor (excluding time and batch dimensions) before any flattening is
applied</dd>
</dl>
<p>In most cases, one of three values should be returned:
1. 'in_shape' if the module does not perform changes to the shape of the data (flattening does not count)
2. some_fixed_shape if the output shape is independent of the input e.g. (out_shape,) for Linear layers
3. 'None' in all other cases (output shape will be inferred automatically)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_abstractmethod
def get_out_shape(self, in_shape):
    &#34;&#34;&#34;
    Args:
        in_shape: data shape of incoming tensor (excluding time and batch dimensions) before any flattening is
            applied

    In most cases, one of three values should be returned:
    1. &#39;in_shape&#39; if the module does not perform changes to the shape of the data (flattening does not count)
    2. some_fixed_shape if the output shape is independent of the input e.g. (out_shape,) for Linear layers
    3. &#39;None&#39; in all other cases (output shape will be inferred automatically)
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="rnnbuilder" href="../index.html">rnnbuilder</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="rnnbuilder.custom.register_non_recurrent" href="#rnnbuilder.custom.register_non_recurrent">register_non_recurrent</a></code></li>
<li><code><a title="rnnbuilder.custom.register_recurrent" href="#rnnbuilder.custom.register_recurrent">register_recurrent</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="rnnbuilder.custom.CustomModule" href="#rnnbuilder.custom.CustomModule">CustomModule</a></code></h4>
<ul class="">
<li><code><a title="rnnbuilder.custom.CustomModule.enter_in_shape" href="#rnnbuilder.custom.CustomModule.enter_in_shape">enter_in_shape</a></code></li>
<li><code><a title="rnnbuilder.custom.CustomModule.forward" href="#rnnbuilder.custom.CustomModule.forward">forward</a></code></li>
<li><code><a title="rnnbuilder.custom.CustomModule.get_initial_output" href="#rnnbuilder.custom.CustomModule.get_initial_output">get_initial_output</a></code></li>
<li><code><a title="rnnbuilder.custom.CustomModule.get_initial_state" href="#rnnbuilder.custom.CustomModule.get_initial_state">get_initial_state</a></code></li>
<li><code><a title="rnnbuilder.custom.CustomModule.get_out_shape" href="#rnnbuilder.custom.CustomModule.get_out_shape">get_out_shape</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>